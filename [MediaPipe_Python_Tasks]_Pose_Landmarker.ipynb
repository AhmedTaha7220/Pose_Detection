{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Pose Landmarks Detection with MediaPipe Tasks\n",
        "\n",
        "This notebook shows you how to use MediaPipe Tasks Python API to detect pose landmarks from images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models) for more information about this model bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYKAJ5nDU8-I"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s3E6NFV-00Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Mega Store\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
        "\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    pose_landmarks_list = detection_result.pose_landmarks\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    # Loop through the detected poses to visualize.\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "        # Draw the pose landmarks.\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend(\n",
        "            [\n",
        "                landmark_pb2.NormalizedLandmark(\n",
        "                    x=landmark.x, y=landmark.y, z=landmark.z\n",
        "                )\n",
        "                for landmark in pose_landmarks\n",
        "            ]\n",
        "        )\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            pose_landmarks_proto,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_styles.get_default_pose_landmarks_style(),\n",
        "        )\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test image\n",
        "\n",
        "To demonstrate the Pose Landmarker API, you can download a sample image using the follow code. The image is from [Pixabay](https://pixabay.com/photos/girl-woman-fitness-beautiful-smile-4051811/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [],
      "source": [
        "# !wget -q -O image.jpg https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg\n",
        "\n",
        "import cv2\n",
        "\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = cv2.imread(\"img.jpg\")\n",
        "# cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-skLwMBmMN_"
      },
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etBjSdwImQPw"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# if len(uploaded.keys()):\n",
        "#   IMAGE_FILE = next(iter(uploaded))\n",
        "#   print('Uploaded file:', IMAGE_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
        "\n",
        "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python) to learn more about configuration options that this solution supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_JVO3rvPD4RN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Mega Store\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "create_from_file(): incompatible function arguments. The following argument types are supported:\n    1. (file_name: str) -> mediapipe.python._framework_bindings.image.Image\n\nInvoked with: array([[[233, 234, 246],\n        [233, 234, 246],\n        [234, 234, 246],\n        ...,\n        [109, 113,  99],\n        [105, 109,  95],\n        [103, 107,  93]],\n\n       [[235, 235, 245],\n        [235, 235, 245],\n        [236, 235, 245],\n        ...,\n        [110, 114, 103],\n        [106, 110,  99],\n        [104, 108,  97]],\n\n       [[237, 236, 243],\n        [237, 236, 243],\n        [237, 236, 243],\n        ...,\n        [111, 113, 107],\n        [107, 109, 103],\n        [105, 107, 101]],\n\n       ...,\n\n       [[134, 123, 130],\n        [132, 120, 127],\n        [127, 116, 123],\n        ...,\n        [ 14,  23,  45],\n        [ 11,  21,  42],\n        [ 10,  20,  41]],\n\n       [[130, 117, 126],\n        [128, 115, 124],\n        [125, 113, 122],\n        ...,\n        [ 18,  24,  46],\n        [ 15,  21,  43],\n        [ 14,  20,  42]],\n\n       [[123, 110, 120],\n        [123, 110, 120],\n        [122, 110, 121],\n        ...,\n        [ 21,  24,  47],\n        [ 18,  21,  44],\n        [ 17,  20,  43]]], dtype=uint8)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m detection_result \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect(image)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# STEP 5: Process the detection result. In this case, visualize it.\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: create_from_file(): incompatible function arguments. The following argument types are supported:\n    1. (file_name: str) -> mediapipe.python._framework_bindings.image.Image\n\nInvoked with: array([[[233, 234, 246],\n        [233, 234, 246],\n        [234, 234, 246],\n        ...,\n        [109, 113,  99],\n        [105, 109,  95],\n        [103, 107,  93]],\n\n       [[235, 235, 245],\n        [235, 235, 245],\n        [236, 235, 245],\n        ...,\n        [110, 114, 103],\n        [106, 110,  99],\n        [104, 108,  97]],\n\n       [[237, 236, 243],\n        [237, 236, 243],\n        [237, 236, 243],\n        ...,\n        [111, 113, 107],\n        [107, 109, 103],\n        [105, 107, 101]],\n\n       ...,\n\n       [[134, 123, 130],\n        [132, 120, 127],\n        [127, 116, 123],\n        ...,\n        [ 14,  23,  45],\n        [ 11,  21,  42],\n        [ 10,  20,  41]],\n\n       [[130, 117, 126],\n        [128, 115, 124],\n        [125, 113, 122],\n        ...,\n        [ 18,  24,  46],\n        [ 15,  21,  43],\n        [ 14,  20,  42]],\n\n       [[123, 110, 120],\n        [123, 110, 120],\n        [122, 110, 121],\n        ...,\n        [ 21,  24,  47],\n        [ 18,  21,  44],\n        [ 17,  20,  43]]], dtype=uint8)"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import cv2\n",
        "\n",
        "# STEP 2: Create an PoseLandmarker object.\n",
        "base_options = python.BaseOptions(model_asset_path=\"pose_landmarker_model.task\")\n",
        "options = vision.PoseLandmarkerOptions(\n",
        "    base_options=base_options, output_segmentation_masks=True\n",
        ")\n",
        "detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "# STEP 3: Load the input image.\n",
        "# mage = mp.Image.create_from_file(\"img.jpg\")\n",
        "\n",
        "# STEP 4: Detect pose landmarks from the input image.\n",
        "# detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.resize(frame, (1280, 720))\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    if not ret:\n",
        "        break\n",
        "    image = mp.Image.create_from_file(frame)\n",
        "    detection_result = detector.detect(image)\n",
        "\n",
        "    # STEP 5: Process the detection result. In this case, visualize it.\n",
        "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "\n",
        "    # Get the width and height of the mediapipe image\n",
        "    width = annotated_image.width()\n",
        "    height = annotated_image.height()\n",
        "\n",
        "    # Convert the mediapipe image to a NumPy array\n",
        "    im = np.frombuffer(annotated_image.tensor(), dtype=np.uint8)\n",
        "    im = im.reshape((height, width, 3))\n",
        "    # face_detector=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    # gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    # detect faces avaliable on camera\n",
        "    # num_faces=face_detector.detectMultiScale(gray_frame,scaleFactor=1.3,minNeighbors=5)\n",
        "    # take each face avaliable on camera and preprocess it\n",
        "\n",
        "    # for(x,y,w,h) in num_faces:\n",
        "    # cv2.rectangle(frame,(x,y-50),(x+w,y+h+10),(0,255,0),4)\n",
        "    # roi_gray_frame=gray_frame[y:y+h,x:x+w]\n",
        "    # cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)),-1),0)\n",
        "    # predict the emotions\n",
        "    # emotion_prediction=emotion_model.predict(cropped_img)\n",
        "    # maxindex=int(np.argmax(emotion_prediction))\n",
        "    # cv2.putText(frame,emotion_dict[maxindex],(x+5,y-20),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2,cv2.LINE_4)\n",
        "    # cv2.imshow(\"im\", im)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "## Setup mediapipe instance\n",
        "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        frame = cv2.resize(frame, (1280, 720))\n",
        "        # Recolor image to RGB\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        image.flags.writeable = False\n",
        "\n",
        "        # Make detection\n",
        "        results = pose.process(image)\n",
        "\n",
        "        # Recolor back to BGR\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Render detections\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image,\n",
        "            results.pose_landmarks,\n",
        "            mp_pose.POSE_CONNECTIONS,\n",
        "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
        "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
        "        )\n",
        "\n",
        "        cv2.imshow(\"Mediapipe Feed\", image)\n",
        "\n",
        "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BwzFvaxwtPX"
      },
      "source": [
        "Visualize the pose segmentation mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jAIFzw9M3JJ"
      },
      "outputs": [],
      "source": [
        "segmentation_mask = detection_result.segmentation_masks[0].numpy_view()\n",
        "visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255\n",
        "cv2_imshow(visualized_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QipRi2ozw7cg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
